{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  N_Days             Drug      Age Sex Ascites Hepatomegaly Spiders  \\\n",
      "0  15000  3492.0              NaN  21185.0   F     NaN          NaN     NaN   \n",
      "1  15001  1654.0              NaN  19724.0   M     NaN          NaN     NaN   \n",
      "2  15002   890.0          Placebo  24621.0   M       N            Y       N   \n",
      "3  15003  1086.0              NaN  18628.0   F     NaN          NaN     NaN   \n",
      "4  15004  4453.0          Placebo  20449.0   F       N            Y       N   \n",
      "5  15005  3086.0  D-penicillamine  15712.0   F       N            N       N   \n",
      "6  15006   611.0          Placebo  26259.0   F       N            Y       Y   \n",
      "7  15007   904.0              NaN  25568.0   F     NaN          NaN     NaN   \n",
      "8  15008  1690.0          Placebo  15574.0   F       N            N       Y   \n",
      "9  15009  1092.0              NaN  21915.0   F     NaN          NaN     NaN   \n",
      "\n",
      "  Edema  Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  \\\n",
      "0     N        0.7          NaN     3.14     NaN       NaN     NaN   \n",
      "1     N        0.6          NaN     3.48     NaN       NaN     NaN   \n",
      "2     N        4.0        244.0     3.02   177.0     960.0   86.00   \n",
      "3     N        0.6          NaN     3.33     NaN       NaN     NaN   \n",
      "4     N        0.9        132.0     3.60    43.0     663.0   52.70   \n",
      "5     N        1.1        364.0     3.40    20.0    2108.0  128.65   \n",
      "6     S        3.6        420.0     3.46   102.0     815.0  127.10   \n",
      "7     N        0.7          NaN     3.40     NaN       NaN     NaN   \n",
      "8     N        1.2        302.0     3.42    51.0    1273.0   72.85   \n",
      "9     N        0.8          NaN     3.81     NaN       NaN     NaN   \n",
      "\n",
      "   Tryglicerides  Platelets  Prothrombin  Stage  \n",
      "0            NaN      269.0         10.6    2.0  \n",
      "1            NaN      306.0         11.1    3.0  \n",
      "2           91.0      360.0         11.0    4.0  \n",
      "3            NaN      246.0         10.6    3.0  \n",
      "4           56.0      344.0         10.6    4.0  \n",
      "5           55.0      298.0         11.4    3.0  \n",
      "6           91.0      195.0         11.5    4.0  \n",
      "7            NaN      255.0         10.2    3.0  \n",
      "8          111.0      430.0          9.7    3.0  \n",
      "9            NaN      312.0         10.6    3.0  \n"
     ]
    }
   ],
   "source": [
    "file_path1 = r\"D:\\.vscode\\Datasets\\train.csv\"\n",
    "file_path2=r\"D:\\.vscode\\Datasets\\test.csv\"\n",
    "\n",
    "# Load the data\n",
    "if os.path.exists(file_path1):\n",
    "    data_train = pd.read_csv(file_path1)\n",
    "    data_test=pd.read_csv(file_path2)\n",
    "    print(data_test.head(10))\n",
    "else:\n",
    "    print(\"File not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.dropna(subset=[\"N_Days\"], inplace=True)\n",
    "data_test.dropna(subset=[\"N_Days\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'N_Days', 'Drug', 'Age', 'Sex', 'Ascites', 'Hepatomegaly',\n",
       "       'Spiders', 'Edema', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper',\n",
       "       'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin',\n",
       "       'Stage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug\n",
      "Sex\n",
      "Ascites\n",
      "Hepatomegaly\n",
      "Spiders\n",
      "Edema\n",
      "Stage\n",
      "Status\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_train.columns)):\n",
    "    col = data_train.columns[i]\n",
    "    if len(data_train[col].unique())<=4:\n",
    "        print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_impute(data, categorical_columns, target_column='Status', id_column=None):\n",
    "    categorical_columns = [col for col in categorical_columns if col in data.columns]\n",
    "    if target_column in categorical_columns:\n",
    "        categorical_columns.remove(target_column)\n",
    "\n",
    "    # Continue with your preprocessing logic\n",
    "    # Keep the target column (Status) separate and don't apply transformations to it\n",
    "    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        if col in categorical_columns or col == target_column:\n",
    "            continue\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            data[col].fillna(data[col].mean(), inplace=True)\n",
    "\n",
    "    # Apply One-Hot Encoding for categorical columns, excluding the target column\n",
    "    data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RK\\AppData\\Local\\Temp\\ipykernel_10288\\3264376635.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].mean(), inplace=True)\n",
      "C:\\Users\\RK\\AppData\\Local\\Temp\\ipykernel_10288\\3264376635.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# List of categorical columns including the target column\n",
    "categorical_columns = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema','Status']\n",
    "\n",
    "# Iterate through each categorical column for imputation\n",
    "preprocessed_data = preprocess_and_impute(data_train, categorical_columns, id_column='id')\n",
    "preprocessed_data_test=preprocess_and_impute(data_test, categorical_columns, id_column='id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'N_Days', 'Age', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper',\n",
      "       'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin',\n",
      "       'Stage', 'Status', 'Drug_Placebo', 'Sex_M', 'Ascites_Y',\n",
      "       'Hepatomegaly_Y', 'Spiders_Y', 'Edema_S', 'Edema_Y'],\n",
      "      dtype='object')\n",
      "Index(['id', 'N_Days', 'Age', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper',\n",
      "       'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin',\n",
      "       'Stage', 'Drug_Placebo', 'Drug_Y', 'Sex_M', 'Ascites_Y',\n",
      "       'Hepatomegaly_Y', 'Spiders_Y', 'Edema_S', 'Edema_Y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_data.columns)  # Check after preprocessing the training data\n",
    "print(preprocessed_data_test.columns)  # Check after preprocessing the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14999 entries, 0 to 14999\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              14999 non-null  int64  \n",
      " 1   N_Days          14999 non-null  float64\n",
      " 2   Age             14999 non-null  float64\n",
      " 3   Bilirubin       14999 non-null  float64\n",
      " 4   Cholesterol     14999 non-null  float64\n",
      " 5   Albumin         14999 non-null  float64\n",
      " 6   Copper          14999 non-null  float64\n",
      " 7   Alk_Phos        14999 non-null  float64\n",
      " 8   SGOT            14999 non-null  float64\n",
      " 9   Tryglicerides   14999 non-null  float64\n",
      " 10  Platelets       14999 non-null  float64\n",
      " 11  Prothrombin     14999 non-null  float64\n",
      " 12  Stage           14999 non-null  float64\n",
      " 13  Status          14999 non-null  object \n",
      " 14  Drug_Placebo    14999 non-null  bool   \n",
      " 15  Sex_M           14999 non-null  bool   \n",
      " 16  Ascites_Y       14999 non-null  bool   \n",
      " 17  Hepatomegaly_Y  14999 non-null  bool   \n",
      " 18  Spiders_Y       14999 non-null  bool   \n",
      " 19  Edema_S         14999 non-null  bool   \n",
      " 20  Edema_Y         14999 non-null  bool   \n",
      "dtypes: bool(7), float64(12), int64(1), object(1)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data['Age_in_Years'] = preprocessed_data['Age'] / 365.25\n",
    "preprocessed_data_test['Age_in_Years'] = preprocessed_data_test['Age'] / 365.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.drop(columns=[\"id\",\"Age\"],axis=1,inplace=True)\n",
    "preprocessed_data_test.drop(columns=[\"id\",\"Age\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14999 entries, 0 to 14999\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   N_Days          14999 non-null  float64\n",
      " 1   Bilirubin       14999 non-null  float64\n",
      " 2   Cholesterol     14999 non-null  float64\n",
      " 3   Albumin         14999 non-null  float64\n",
      " 4   Copper          14999 non-null  float64\n",
      " 5   Alk_Phos        14999 non-null  float64\n",
      " 6   SGOT            14999 non-null  float64\n",
      " 7   Tryglicerides   14999 non-null  float64\n",
      " 8   Platelets       14999 non-null  float64\n",
      " 9   Prothrombin     14999 non-null  float64\n",
      " 10  Stage           14999 non-null  float64\n",
      " 11  Status          14999 non-null  object \n",
      " 12  Drug_Placebo    14999 non-null  bool   \n",
      " 13  Sex_M           14999 non-null  bool   \n",
      " 14  Ascites_Y       14999 non-null  bool   \n",
      " 15  Hepatomegaly_Y  14999 non-null  bool   \n",
      " 16  Spiders_Y       14999 non-null  bool   \n",
      " 17  Edema_S         14999 non-null  bool   \n",
      " 18  Edema_Y         14999 non-null  bool   \n",
      " 19  Age_in_Years    14999 non-null  float64\n",
      "dtypes: bool(7), float64(12), object(1)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C', 'D', 'CL'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[\"Status\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['N_Days', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos',\n",
       "       'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage', 'Status',\n",
       "       'Drug_Placebo', 'Sex_M', 'Ascites_Y', 'Hepatomegaly_Y', 'Spiders_Y',\n",
       "       'Edema_S', 'Edema_Y', 'Age_in_Years'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Age_in_Years', 'Cholesterol', 'Bilirubin', 'Prothrombin', 'Copper', 'SGOT']\n",
    "X_train = preprocessed_data[features]\n",
    "y_train = preprocessed_data['Status']\n",
    "X_test = preprocessed_data_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(data, target_column='Status'):\n",
    "    # Separate target column and features\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Standardize features (excluding the target column)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(X_train, y_train):\n",
    "    # Define the models and hyperparameters to tune\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(),\n",
    "        'SVM': SVC(probability=True),\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'DecisionTree': DecisionTreeClassifier(),\n",
    "        'NaiveBayes': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    param_grid = {\n",
    "        'RandomForest': {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15]},\n",
    "        'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "        'LogisticRegression': {'C': [0.1, 1, 10]},\n",
    "        'KNN': {'n_neighbors': [3, 5, 7]},\n",
    "        'DecisionTree': {'max_depth': [5, 10, 15]},\n",
    "        'NaiveBayes': {}\n",
    "    }\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    \n",
    "    # Perform GridSearchCV for each model and find the best one\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Tuning {model_name}...\")\n",
    "        grid = GridSearchCV(model, param_grid[model_name], cv=3, scoring='accuracy')\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        # Update the best model if current one has a better score\n",
    "        if grid.best_score_ > best_score:\n",
    "            best_score = grid.best_score_\n",
    "            best_model = grid.best_estimator_\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data['Status'] = preprocessed_data['Status'].map({'C': 0, 'CL': 1, 'D': 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\.vscode\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning SVM...\n"
     ]
    }
   ],
   "source": [
    "def create_submission_file(model, data):\n",
    "    X = data.drop(columns=['Status'])\n",
    "    # Standardize the features\n",
    "    X_scaled, _ = standardize_data(preprocessed_data, target_column='Status')\n",
    "    \n",
    "    # Get the predicted probabilities\n",
    "    probabilities = model.predict_proba(X_scaled)\n",
    "    \n",
    "    # Create a submission DataFrame\n",
    "    submission = pd.DataFrame(probabilities, columns=['Statusc', 'Statuscl', 'Statusd'])\n",
    "    submission['id'] = data['id']\n",
    "    \n",
    "    # Shuffle the DataFrame if needed\n",
    "    submission = shuffle(submission)\n",
    "    \n",
    "    # Save to CSV\n",
    "    submission.to_csv('submission_file.csv', index=False)\n",
    "    print(\"Submission file saved as 'submission_file.csv'\")\n",
    "\n",
    "# Main Execution Flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Hyperparameter Tuning and Model Training\n",
    "    best_model = hyperparameter_tuning(X_train, y_train)\n",
    "\n",
    "    # Step 2: Get predicted probabilities for the test set\n",
    "    probabilities = evaluate_model(best_model, X_test)\n",
    "    \n",
    "    # Step 3: Create and Save Submission File\n",
    "    create_submission_file(best_model, preprocessed_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_data_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
